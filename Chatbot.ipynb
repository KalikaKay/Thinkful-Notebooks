{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chatbot.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOjYsoArNg6NUOL/prTIAh5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KalikaKay/Thinkful-Notebooks/blob/main/Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0Uek62GbS_A"
      },
      "source": [
        "#Setup dataframe and visualizations\n",
        "import math\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.stats.mstats import winsorize\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "import matplotlib.pyplot as plt\n",
        "import datetime \n",
        "\n",
        "\n",
        "#Figure set up for dark theme:\n",
        "plt.style.use(['dark_background'])\n",
        "#Color to set all my graphs.\n",
        "color = '#F9EDF5'\n",
        "sns.set()\n",
        "\n",
        "#suppress warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MsHSXTvZbbU_"
      },
      "source": [
        "#Natural Language Dating Cleaning Import \r\n",
        "from collections import Counter\r\n",
        "import nltk\r\n",
        "import spacy\r\n",
        "import re"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQnQFLZiQBqc"
      },
      "source": [
        "# Build a Chatbot\r\n",
        "\r\n",
        "First, do some data preprocessing to clean up the data. You can use your solution to the assignment of the Text preprocessing checkpoint."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5FANV6FY7tA"
      },
      "source": [
        "# Natural Language Processing (NLP) \r\n",
        "## Data Cleaning\r\n",
        "\r\n",
        "*Note: Because the memory requirements of the datasets are relatively large, it's best to use Google Colaboratory for this assignment.*\r\n",
        "\r\n",
        "Note: When parsing the data using spaCy, you may run into some memory issues, even in Google Colaboratory. If you're having memory issues, try parsing your text as follows:\r\n",
        "\r\n",
        "```\r\n",
        "nlp = spacy.load('en', disable=['parser', 'ner'])\r\n",
        "nlp.add_pipe(nlp.create_pipe('sentencizer'))\r\n",
        "nlp.max_length = 20000000\r\n",
        "doc = nlp(the_dialogs_come_here)\r\n",
        "```\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9MS0ToQZBtA"
      },
      "source": [
        "## Cornell Movie--Dialogs Corpus.\r\n",
        "\r\n",
        "The first dataset is a dialogue dataset called Cornell Movie--Dialogs Corpus. This corpus includes conversations between the characters of more than 600 movies.\r\n",
        "\r\n",
        "Apply the data preprocessing techniques that you learned here to the Cornell Movie--Dialogs Corpus data. \r\n",
        "\r\n",
        "You'll use this dataset when developing a chatbot in an upcoming checkpoint. \r\n",
        "\r\n",
        "Access the dataset from the Thinkful database using the following credentials:\r\n",
        "\r\n",
        " \r\n",
        "\r\n",
        "```\r\n",
        " postgres_user = 'dsbc_student'\r\n",
        " postgres_pw = '7*.8G9QH21'\r\n",
        " postgres_host = '142.93.121.174'\r\n",
        " postgres_port = '5432'\r\n",
        " postgres_db = 'cornell_movie_dialogs'\r\n",
        "# The data is in the table called dialogs.\r\n",
        "```\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOCxnITvaSQT"
      },
      "source": [
        "from sqlalchemy import create_engine\n",
        "def get_df(table, database=None):\n",
        "\n",
        "  postgres_user = 'dsbc_student'\n",
        "  postgres_pw = '7*.8G9QH21'\n",
        "  postgres_host = '142.93.121.174'\n",
        "  postgres_port = '5432'\n",
        "\n",
        "  if database is None:\n",
        "    database = table\n",
        "  # INSERT WITH DB NAME\n",
        "  postgres_db = database\n",
        "  dt = table\n",
        "\n",
        "  # ASSUMED SQL STATEMENT, UPDATE AS NECESSARY.\n",
        "  sql = 'select * from ' + dt\n",
        "\n",
        "  engine = create_engine('postgresql://{}:{}@{}:{}/{}'.format(\n",
        "      postgres_user, postgres_pw, postgres_host, postgres_port, postgres_db))\n",
        "\n",
        "  df = pd.read_sql_query(sql,con=engine)\n",
        "\n",
        "  # no need for an open connection, \n",
        "  # as we're only doing a single query\n",
        "  engine.dispose()\n",
        "\n",
        "  return df"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "jqED9uFFY19L",
        "outputId": "08a3e406-c3a0-4e1c-d732-6c9fbb972332"
      },
      "source": [
        "mov = get_df('dialogs', 'cornell_movie_dialogs')\r\n",
        "mov.drop(columns='index', inplace=True)\r\n",
        "mov.head()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>dialogs</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Can we make this quick?  Roxanne Korrine and A...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Well, I thought we'd start with pronunciation,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Not the hacking and gagging and spitting part....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Okay... then how 'bout we try out some French ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>You're asking me out.  That's so cute. What's ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             dialogs\n",
              "0  Can we make this quick?  Roxanne Korrine and A...\n",
              "1  Well, I thought we'd start with pronunciation,...\n",
              "2  Not the hacking and gagging and spitting part....\n",
              "3  Okay... then how 'bout we try out some French ...\n",
              "4  You're asking me out.  That's so cute. What's ..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5DAFTGsvviJ"
      },
      "source": [
        "The steps below were pulled from [here](https://prrao87.github.io/blog/spacy/nlp/performance/2020/05/02/spacy-multiprocess.html). I was looking at - wanted to look at - a few alternative cleaning techniques. \r\n",
        "\r\n",
        "After taking a look at spacy's stop_word results (info not saved, just take my own word for it); I found that I prefer to use this cleaning method prior to running my data through the natural language processor. \r\n",
        "\r\n",
        "This function could be updated. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NaWjI3l_q2Gb"
      },
      "source": [
        "#Clean the data frame\r\n",
        "def cleaner(df, field, limit=0):\r\n",
        "    \"Extract relevant text from DataFrame using a regex\"\r\n",
        "    # Regex pattern for only alphanumeric, hyphenated text with 3 or more chars\r\n",
        "    pattern = re.compile(r\"[A-Za-z0-9\\-]{3,50}\")\r\n",
        "    df['clean'] = df[field].str.findall(pattern).str.join(' ')\r\n",
        "    if limit > 0:\r\n",
        "        return df.iloc[:limit, :].copy()\r\n",
        "    else:\r\n",
        "        return df"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Xk63BIANckK"
      },
      "source": [
        "# Utility function for standard text cleaning\r\n",
        "def text_cleaner(text):\r\n",
        "    type(text)\r\n",
        "    # Visual inspection identifies a form of punctuation that spaCy does not\r\n",
        "    # recognize: the double dash --.  Better get rid of it now!\r\n",
        "    text = re.sub(r'--',' ',text)\r\n",
        "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\r\n",
        "    text = re.sub(r\"(\\b|\\s+\\-?|^\\-?)(\\d+|\\d*\\.\\d+)\\b\", \" \", text)\r\n",
        "    text = ' '.join(text.split())\r\n",
        "    return text"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBUC_pyfusZz"
      },
      "source": [
        "nlp = spacy.load('en')"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ih2FdoCczV6C"
      },
      "source": [
        "mov['clean'] = cleaner(mov, 'dialogs')"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R6ccYeI1vAYu",
        "outputId": "369f44aa-d282-456d-f933-0bed9d455d31"
      },
      "source": [
        "dialogs = []\r\n",
        "#i = 1\r\n",
        "print(\"Processing {} elements. start time: {}\".format(mov['dialogs'].count(), datetime.datetime.now()))\r\n",
        "for dialog in mov['dialogs']:\r\n",
        "  #print(\"item {} start time: {}\".format(i, datetime.datetime.now()))\r\n",
        "  dialog = text_cleaner(dialog)\r\n",
        "  dialogs.append(nlp(dialog))\r\n",
        "  #i += 1\r\n",
        "print(\"End time: {}\".format(datetime.datetime.now()))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing 304446 elements. start time: 2020-12-21 21:38:01.020571\n",
            "End time: 2020-12-21 22:22:37.778316\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7RYTnwJYw11T",
        "outputId": "c7b58efe-c221-4ac8-f5b9-14cd598079b7"
      },
      "source": [
        "# Explore the objects that you've built.\r\n",
        "print(\"Object Type: {}\".format(type(dialogs)))\r\n",
        "print(\"Object Length: {} \".format(len(dialogs)))\r\n",
        "print(\"First Three: {}'\".format(dialogs[:3]))\r\n",
        "print(\"Elements Type: {}\".format(type(dialogs[0])))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Object Type: <class 'list'>\n",
            "Object Length: 304446 \n",
            "First Three: [Can we make this quick? Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad. Again., Well, I thought we'd start with pronunciation, if that's okay with you., Not the hacking and gagging and spitting part. Please.]'\n",
            "Elements Type: <class 'spacy.tokens.doc.Doc'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvfQNOKFP65Q"
      },
      "source": [
        "Develop a chatbot using this corpus. In doing this, you're free to choose a chatbot development library like ChatterBot or write your own code from scratch.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGlLIXIKP57P"
      },
      "source": [
        "import random\r\n",
        "GREETING_INPUTS = [\"hello\", \"hi\", \"greetings\", \"what's up\",\"hey\"]\r\n",
        "GREETING_RESPONSES = [\"hello\", \"hi\", \"hey\", \"hi there\", \"howdy\"]\r\n",
        "def greeting(sentence):\r\n",
        "    for word in sentence.split():\r\n",
        "        if word.lower() in GREETING_INPUTS:\r\n",
        "            return random.choice(GREETING_RESPONSES)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fNPlGDva-6m"
      },
      "source": [
        "def response(user_input, persuasion_sents):\r\n",
        "    \r\n",
        "    response = \"\"\r\n",
        "    # Use spaCy to parse the user's input\r\n",
        "    input_doc = nlp(user_input)\r\n",
        "    # Then split it into sentences\r\n",
        "    input_sents = [sent.text for sent in input_doc.sents]\r\n",
        "\r\n",
        "    # Then append the user's sentence into your list of sentences\r\n",
        "    for sentence in input_sents:\r\n",
        "        persuasion_sents.append(sentence)\r\n",
        "    \r\n",
        "    # The next step is to vectorize your new corpus using TF-IDF\r\n",
        "    TfidfVec = TfidfVectorizer(max_df=0.5, min_df=1, use_idf=True, norm=u'l2', smooth_idf=True, lowercase=False)\r\n",
        "    tfidf = TfidfVec.fit_transform(persuasion_sents)\r\n",
        "    \r\n",
        "    # Remove the user's input from the corpus\r\n",
        "    persuasion_sents.pop(-1)\r\n",
        "    \r\n",
        "    # Calculate the cosine similarity\r\n",
        "    # between the user input and all of the other sentences in the corpus\r\n",
        "    similarities = cosine_similarity(tfidf[-1], tfidf[:-1])\r\n",
        "    # Get the index of most similar sentence\r\n",
        "    idx = np.argmax(similarities)\r\n",
        "        \r\n",
        "    if(idx):\r\n",
        "        response = response + persuasion_sents[idx]\r\n",
        "        return response\r\n",
        "    else:\r\n",
        "        response = response + \"I'm sorry! I don't know how to respond :(\"\r\n",
        "        return response"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWX7GZtxh_de"
      },
      "source": [
        "vocabulary = []\r\n",
        "for dialog in dialogs:\r\n",
        "  if type(dialog) == str:\r\n",
        "    vocabulary.append(dialog)\r\n",
        "  else: \r\n",
        "    vocabulary.append(dialog.text)"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRjIE-mmczO7",
        "outputId": "eb52706c-c3bc-47f6-854e-669a8ee5a41e"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "from sklearn.metrics.pairwise import cosine_similarity\r\n",
        "\r\n",
        "print(\"The Dialogian: I will try to respond to you reasonably. If you want to exit, type bye.\")\r\n",
        "\r\n",
        "while(True):\r\n",
        "    \r\n",
        "    user_input = input(\"User: \")\r\n",
        "    user_input=user_input.lower()\r\n",
        "    \r\n",
        "    if(user_input!='bye'):\r\n",
        "        if(user_input == 'thanks' or user_input == 'thank you'):\r\n",
        "            break\r\n",
        "            print(\"Persuasion: You're welcome.\")\r\n",
        "        else:\r\n",
        "            if(greeting(user_input) != None):\r\n",
        "                print(\"Persuasion: \" + greeting(user_input))\r\n",
        "            else:\r\n",
        "                print(\"Persuasion: \", end = \"\")\r\n",
        "                print(response(user_input, vocabulary))\r\n",
        "    else:\r\n",
        "        print(\"Persuasion: Bye! It was a great chat.\")\r\n",
        "        break"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Dialogian: I will try to respond to you reasonably. If you want to exit, type bye.\n",
            "User: hello\n",
            "Persuasion: howdy\n",
            "User: how are you today?\n",
            "Persuasion: how are you today?\n",
            "User: what?\n",
            "Persuasion: A what?\n",
            "User: it's cold outside.\n",
            "Persuasion: I'm cold.\n",
            "User: merry christmas!\n",
            "Persuasion: A merry Christmas to you, Amy.\n",
            "User: who is Amy?\n",
            "Persuasion: ...You know... who *who* is?\n",
            "User: you're so funny. I'm Kalika. What's your name?\n",
            "Persuasion: And what is your name?\n",
            "User: kalika\n",
            "Persuasion: i'm kalika.\n",
            "User: You've stolen my identity!\n",
            "Persuasion: It was stolen!\n",
            "User: Give it back!\n",
            "Persuasion: Okay, give it back.\n",
            "User: now. what is  your name?\n",
            "Persuasion: And what is your name?\n",
            "User: jon. who are you?\n",
            "Persuasion: I know who you are.\n",
            "User: you're terrible is who you are! absolutely terrible. By!\n",
            "Persuasion: ...by you?\n",
            "User: no. i mean, good bye.\n",
            "Persuasion: ...good-bye then.\n",
            "User: bye\n",
            "Persuasion: Bye! It was a great chat.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YW2goZTiQS17"
      },
      "source": [
        "Start a conversation with your chatbot, and discuss its strengths and weaknesses.\r\n",
        "\r\n",
        "It's a funny little bot. It cannot identify itself, but it does have a little bit of character. Of course there's work to be d one, but all in all - it's cute. I had a lot of fun with it. \r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTC-n6PdkYIs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}