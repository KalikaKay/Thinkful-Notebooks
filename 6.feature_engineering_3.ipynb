{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zyMBrDwJnyd3"
   },
   "source": [
    "In a nutshell, the word2vec algorithm takes a corpus of text as input and generates a vector of several hundred dimensions for each word in the corpus. In doing this, it doesn't rely on the counts of the occurrences of the words. Instead, it follows a much more complicated procedure that considers the surrounding words of each word in the sentences.\n",
    "\n",
    "Before moving further, note that you're still in the feature-engineering step as shown below:\n",
    "\n",
    "![Feature Engineering](assets/feature_engineering.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uCTjtARdnyd4",
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "# What is word2vec?\n",
    "\n",
    "The most common feature-generation approach for NLP tasks is word2vec. What word2vec does is that it trains a shallow neural network model in an unsupervised manner for converting words to vectors. At the highest level of abstraction, word2vec assigns a vector of random values to each word. For a word *W*, it looks at the words that are near *W* in the sentence. It then shifts the values in the word vectors, such that the vectors for words near *W* are closer to the *W* vector, and vectors for words not near *W* are farther away from the *W* vector. With a large enough corpus, this will eventually result in words that often appear together having vectors that are near one another, and words that rarely or never appear together having vectors that are far away from each other.\n",
    "\n",
    "This may sound quite similar to the latent semantic analysis approach that you learned about in the previous checkpoint. The conceptual difference is that LSA creates vector representations of sentences based on the words in them, while word2vec creates representations of individual words, based on the words around them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_zyjblsFnyd5"
   },
   "source": [
    "## What is it good for?\n",
    "\n",
    "Word2vec is strong at capturing the meanings of the words, so it's also good at detecting words that have similar meanings. The challenge with human communication is that there are many different ways to communicate the same concept. It's easy for humans to know that `the silverware` and `the utensils` can refer to the same thing. But computers can't do that unless you teach them, and this can be a real choke point for human-computer interactions. If you've ever played a text adventure game like *Colossal Cave Adventure* or *Zork*, you may have encountered the following scenario:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "raw",
    "id": "esovvnnhnyd7"
   },
   "source": [
    "    GAME: You are on a forest path north of the field. A cave leads into a granite butte to the north.\n",
    "    A thick hedge blocks the way to the west.\n",
    "    A hefty stick lies on the ground.\n",
    "\n",
    "    YOU: pick up stick  \n",
    "\n",
    "    GAME: You don't know how to do that.  \n",
    "\n",
    "    YOU: lift stick  \n",
    "\n",
    "    GAME: You don't know how to do that.  \n",
    "\n",
    "    YOU: take stick  \n",
    "\n",
    "    GAME: You don't know how to do that.  \n",
    "\n",
    "    YOU: grab stick  \n",
    "\n",
    "    GAME: You grab the stick from the ground and put it in your bag.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VOVMQrdxnyd8"
   },
   "source": [
    "And your brain explodes from frustration. A text adventure game that incorporates a properly trained word2vec model would have vectors for `pick up`, `lift`, and `take` that are close to the vector for `grab`. Therefore, it could accept those other verbs as synonyms so that you could move ahead faster. In more practical applications, word2vec and other similar algorithms are what help a search engine return the best results for your query, not just the results that contain the exact words that you used. In fact, a search is a better example. Not only does the search engine need to understand your request, it also needs to match it to web pages that were also written by humans and therefore also use idiosyncratic language.\n",
    "\n",
    "Next, look very briefly at the word2vec algorithm and examine how it comes up with vector representations of words that capture semantics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6rgG5EApnyd8"
   },
   "source": [
    "## Generating vectors: Multiple algorithms\n",
    "\n",
    "In considering the relationship between a word and its surrounding words, word2vec has two options that are the inverse of one another:\n",
    "\n",
    " * **Continuous bag of words (CBOW):** The identity of a word is predicted using the words near it in a sentence.\n",
    " * **Skip-gram:** The identities of words are predicted from the word that they surround. Skip-gram seems to work better for larger corpora.\n",
    "\n",
    "Now, consider the following sentence:\n",
    "    \n",
    "    \"Terry Gilliam is a better comedian than a director\" \n",
    "\n",
    "Focus on the word `comedian` here. CBOW will try to predict `comedian` using `is`, `a`, `better`, `than`, `a`, and `director`. Skip-gram will try to predict `is`, `a`, `better`, `than`, `a`, and `director` using the word `comedian`. In practice, for CBOW, the vector for `comedian` will be pulled closer to the other words. But for skip-gram, the vectors for the other words will be pulled closer to `comedian`.  \n",
    "\n",
    "In addition to moving the vectors for nearby words closer together, each time a word is processed, some vectors are moved farther away. Word2vec has two approaches to pushing vectors apart:\n",
    " \n",
    " * **Negative sampling:** Like it sounds, each time that a word is pulled toward some neighbors, the vectors for a randomly chosen small set of other words are pushed away.\n",
    " * **Hierarchical softmax:** Every neighboring word is pulled closer or farther from a subset of words chosen based on a tree of probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZqRBWBKinyd9"
   },
   "source": [
    "## What is similarity? Strengths and weaknesses of word2vec\n",
    "\n",
    "Keep in mind that word2vec operates on the assumption that frequent proximity indicates similarity, but words can be similar in various ways. They may be conceptually similar (`royal`, `king`, and `throne`), but they may also be functionally similar (`tremendous` and `negligible` are both common modifiers of `size`). Here is a more [detailed exploration, with examples](https://quomodocumque.wordpress.com/2016/01/15/messing-around-with-word2vec/), of what similarity means in word2vec.\n",
    "\n",
    "One cool thing about word2vec is that it can identify similarities between words that never occur near one another in the corpus. For example, consider these sentences:\n",
    "\n",
    "    \"The dog played with an elastic ball.\"\n",
    "    \"Babies prefer the ball that is bouncy.\"\n",
    "    \"I wanted to find a ball that's elastic.\"\n",
    "    \"Tracy threw a bouncy ball.\"\n",
    "\n",
    "`Elastic` and `bouncy` are similar in meaning in the text but don't appear in the same sentence. However, both appear near `ball`. In the process of nudging the vectors around so that `elastic` and `bouncy` are both near the vector for `ball`, the words also become nearer to one another and their similarity can be detected.\n",
    "\n",
    "For a while, after it was introduced, [no one was really sure why word2vec worked as well as it did](https://arxiv.org/pdf/1402.3722v1.pdf) (see the last paragraph of the linked paper). A few years later, some additional math was developed to explain word2vec and similar models. If you are comfortable with both math and academic writing, have a lot of time on your hands, and want to take a deep dive into the inner workings of word2vec, [check out this paper](https://arxiv.org/pdf/1502.03520v7.pdf) from 2016.\n",
    "\n",
    "One of the draws of word2vec when it first came out was that the vectors could be used to convert analogies (`king` is to `queen` as `man` is to `woman`, for example) into mathematical expressions (`king` + `woman` - `man` = ?) and solve for the missing element (`queen`). This is kind of nifty.\n",
    "\n",
    "A drawback of word2vec is that it works best with a corpus that is at least several billion words long. Even though the word2vec algorithm is speedy, this is a lot of data and takes a long time! In the following examples, your dataset is very short. This allows you to run it in the Notebook without overwhelming the kernel, but probably won't give great results. Still, you'll explore how you can implement word2vec using the Gensim library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lTYW9SDTnyd-"
   },
   "source": [
    "## Implementing word2vec\n",
    "\n",
    "Now, you can start to use word2vec representations of the words to feed into machine-learning models. There are a few word2vec implementations in Python, but the general consensus is that the easiest one to use is [Gensim](https://radimrehurek.com/gensim/models/word2vec.html). Now is a good time to install this library if you don't have it yet. Install it as follows:\n",
    "\n",
    "```bash\n",
    "pip install gensim\n",
    "````\n",
    "\n",
    "In the following examples, you'll use the Gensim library along with others. As you did in the previous checkpoints, you'll be working on Jane Austen's *Persuasion* and Lewis Carroll's *Alice's Adventures in Wonderland*.\n",
    "\n",
    "You have two options when working with the word2vec vectors in Gensim. The first one is to train your own word2vec algorithm using your own corpus. This will be your first approach in the following process. However, for the word2vec algorithm to perform well, you need a much larger corpus than you have here. So, the second option is to load a pretrained word2vec vector that's been trained on a very large corpus. After you train your word2vec representations, you'll also load a pretrained one.\n",
    "\n",
    "Now, start with importing the libraries that you'll use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190
    },
    "colab_type": "code",
    "id": "QajclIwjnyeA",
    "outputId": "32c6c2ae-4596-4789-e129-3d1eb92331b9",
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import en_core_web_sm as spacy\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "import gensim\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#nltk.download('gutenberg')\n",
    "#!python -m spacy download en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qt6pE-RAnyeE"
   },
   "source": [
    "Before moving on to vectorizing the text, you need to clean your data. You can use the same cleaning codes as in the previous checkpoints, because you're using the same documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GStl5G-gnyeF"
   },
   "outputs": [],
   "source": [
    "# Utility function for standard text cleaning\n",
    "def text_cleaner(text):\n",
    "    # Visual inspection identifies a form of punctuation that spaCy doesn't\n",
    "    # recognize: the double dash --. Better get rid of it now!\n",
    "    text = re.sub(r'--',' ',text)\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "    text = re.sub(r\"(\\b|\\s+\\-?|^\\-?)(\\d+|\\d*\\.\\d+)\\b\", \" \", text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fYGkgFujnyeI"
   },
   "outputs": [],
   "source": [
    "# Load and clean the data\n",
    "persuasion = gutenberg.raw('austen-persuasion.txt')\n",
    "alice = gutenberg.raw('carroll-alice.txt')\n",
    "\n",
    "# The chapter indicator is idiosyncratic\n",
    "persuasion = re.sub(r'Chapter \\d+', '', persuasion)\n",
    "alice = re.sub(r'CHAPTER .*', '', alice)\n",
    "    \n",
    "alice = text_cleaner(alice)\n",
    "persuasion = text_cleaner(persuasion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xw1gGshfnyeJ"
   },
   "outputs": [],
   "source": [
    "# Parse the cleaned novels. This can take some time.\n",
    "nlp = spacy.load()\n",
    "alice_doc = nlp(alice)\n",
    "persuasion_doc = nlp(persuasion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "nVRmVxehnyeL",
    "outputId": "0aa6d425-1bc2-45a2-875a-4ca7090ed647"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(Alice, was, beginning, to, get, very, tired, ...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(So, she, was, considering, in, her, own, mind...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(There, was, nothing, so, VERY, remarkable, in...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(Oh, dear, !)</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(Oh, dear, !)</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text   author\n",
       "0  (Alice, was, beginning, to, get, very, tired, ...  Carroll\n",
       "1  (So, she, was, considering, in, her, own, mind...  Carroll\n",
       "2  (There, was, nothing, so, VERY, remarkable, in...  Carroll\n",
       "3                                      (Oh, dear, !)  Carroll\n",
       "4                                      (Oh, dear, !)  Carroll"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group into sentences\n",
    "alice_sents = [[sent, \"Carroll\"] for sent in alice_doc.sents]\n",
    "persuasion_sents = [[sent, \"Austen\"] for sent in persuasion_doc.sents]\n",
    "\n",
    "# Combine the sentences from the two novels into one DataFrame\n",
    "sentences = pd.DataFrame(alice_sents + persuasion_sents, columns = [\"text\", \"author\"])\n",
    "sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wf7Wk2NwnyeN"
   },
   "outputs": [],
   "source": [
    "# Get rid of stop words and punctuation,\n",
    "# and lemmatize the tokens\n",
    "for i, sentence in enumerate(sentences[\"text\"]):\n",
    "    sentences.loc[i, \"text\"] = [token.lemma_ for token in sentence if not token.is_punct and not token.is_stop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DNPcJeLmnyeP"
   },
   "source": [
    "Now, you're ready to vectorize your words using word2vec. For this purpose, use `Word2Vec` from Gensim's `models` module. The `Word2Vec` class has several parameters. Set the following parameters:\n",
    "\n",
    "* `workers=4`: Set the number of threads to run in parallel to 4 (which makes sense if your computer has available computing units).\n",
    "* `min_count=1`: Set the minimum word count threshold to 1.\n",
    "* `window=6`: Set the number of words around the target word to consider to 6.\n",
    "* `sg=0`: Use CBOW because your corpus is small.\n",
    "* `sample=1e-3`: Penalize frequent words.\n",
    "* `size=100`: Set the word vector length to 100.\n",
    "* `hs=1`: Use hierarchical softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CaHMx8TfnyeQ"
   },
   "outputs": [],
   "source": [
    "# Train word2vec on the sentences\n",
    "model = gensim.models.Word2Vec(\n",
    "    sentences[\"text\"],\n",
    "    workers=4,\n",
    "    min_count=1,\n",
    "    window=6,\n",
    "    sg=0,\n",
    "    sample=1e-3,\n",
    "    size=100,\n",
    "    hs=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c_39am1FnyeT"
   },
   "source": [
    "Before jumping into the machine-learning model for prediction, play with the word2vec word representation that you just trained. Specifically, look into the following:\n",
    "\n",
    "* The first five words that are closer to `lady`\n",
    "* The word that doesn't fit in this list: `dad`, `dinner`, `mom`, `aunt`, `uncle`\n",
    "* The similarity score of `woman` and `man`\n",
    "* The similarity score of `horse` and `cat`\n",
    "\n",
    "Note that all of the above calculations are based on the word2vec representations of the words that you just trained above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "qRN8FY92nyeT",
    "outputId": "e9ed3e27-03bd-4c11-c646-fd5220d7a465"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('young', 0.9989365339279175), ('chance', 0.9985891580581665), ('admit', 0.9984226226806641), ('ought', 0.9982870817184448), ('soon', 0.998206377029419)]\n",
      "dinner\n",
      "0.99702954\n",
      "0.9535763\n"
     ]
    }
   ],
   "source": [
    "print(model.most_similar(positive=['lady', 'man'], negative=['woman'], topn=5))\n",
    "print(model.doesnt_match(\"dad dinner mom aunt uncle\".split()))\n",
    "print(model.similarity('woman', 'man'))\n",
    "print(model.similarity('horse', 'cat'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XAPrzFiEnyeV"
   },
   "source": [
    "Well, the results make sense to some degree, but it's obvious that your representations aren't perfect. This is because your corpus is small. To get more meaningful results, you need to train word2vec representations using much larger corpora.\n",
    "\n",
    "Now, create your numerical features using the word2vec representations of the words. In the following, get the word2vec vectors of each word in a sentence. Then take the average of all the vectors in the high dimensional space (in your case, it's 100). So, as a result, you'll have a vector of 100 dimensions as the feature for a sentence. You can then use each dimension as a separate feature—which means that you'll have 100 numerical features in your final dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 440
    },
    "colab_type": "code",
    "id": "5avMNgVXnyeW",
    "outputId": "f3d3aa1c-e085-4ffe-de40-4a057cc21608",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Carroll</td>\n",
       "      <td>[Alice, begin, tired, sit, sister, bank, have,...</td>\n",
       "      <td>-0.411962</td>\n",
       "      <td>-0.530732</td>\n",
       "      <td>0.088587</td>\n",
       "      <td>-0.269412</td>\n",
       "      <td>-0.255319</td>\n",
       "      <td>0.379318</td>\n",
       "      <td>-0.398875</td>\n",
       "      <td>-0.146914</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.070243</td>\n",
       "      <td>0.043718</td>\n",
       "      <td>-0.056762</td>\n",
       "      <td>0.201070</td>\n",
       "      <td>-0.036714</td>\n",
       "      <td>-0.039553</td>\n",
       "      <td>0.271948</td>\n",
       "      <td>-0.173071</td>\n",
       "      <td>-0.046284</td>\n",
       "      <td>-0.668204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Carroll</td>\n",
       "      <td>[consider, mind, hot, day, feel, sleepy, stupi...</td>\n",
       "      <td>-0.323007</td>\n",
       "      <td>-0.434219</td>\n",
       "      <td>0.066623</td>\n",
       "      <td>-0.247031</td>\n",
       "      <td>-0.199145</td>\n",
       "      <td>0.322621</td>\n",
       "      <td>-0.328407</td>\n",
       "      <td>-0.137773</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.065762</td>\n",
       "      <td>0.041397</td>\n",
       "      <td>-0.033571</td>\n",
       "      <td>0.185754</td>\n",
       "      <td>-0.036454</td>\n",
       "      <td>-0.037165</td>\n",
       "      <td>0.237803</td>\n",
       "      <td>-0.157099</td>\n",
       "      <td>-0.047586</td>\n",
       "      <td>-0.555798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Carroll</td>\n",
       "      <td>[remarkable, Alice, think, way, hear, Rabbit]</td>\n",
       "      <td>-0.492145</td>\n",
       "      <td>-0.664988</td>\n",
       "      <td>0.097490</td>\n",
       "      <td>-0.330539</td>\n",
       "      <td>-0.317640</td>\n",
       "      <td>0.492857</td>\n",
       "      <td>-0.513320</td>\n",
       "      <td>-0.171587</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.093407</td>\n",
       "      <td>0.049211</td>\n",
       "      <td>-0.062314</td>\n",
       "      <td>0.260365</td>\n",
       "      <td>-0.041242</td>\n",
       "      <td>-0.031419</td>\n",
       "      <td>0.339620</td>\n",
       "      <td>-0.214381</td>\n",
       "      <td>-0.052508</td>\n",
       "      <td>-0.825797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Carroll</td>\n",
       "      <td>[oh, dear]</td>\n",
       "      <td>-0.432766</td>\n",
       "      <td>-0.551242</td>\n",
       "      <td>0.065667</td>\n",
       "      <td>-0.326656</td>\n",
       "      <td>-0.250697</td>\n",
       "      <td>0.415931</td>\n",
       "      <td>-0.445387</td>\n",
       "      <td>-0.169576</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.084803</td>\n",
       "      <td>0.022198</td>\n",
       "      <td>-0.004809</td>\n",
       "      <td>0.226827</td>\n",
       "      <td>-0.036816</td>\n",
       "      <td>-0.034167</td>\n",
       "      <td>0.277438</td>\n",
       "      <td>-0.199405</td>\n",
       "      <td>-0.047371</td>\n",
       "      <td>-0.697044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Carroll</td>\n",
       "      <td>[oh, dear]</td>\n",
       "      <td>-0.432766</td>\n",
       "      <td>-0.551242</td>\n",
       "      <td>0.065667</td>\n",
       "      <td>-0.326656</td>\n",
       "      <td>-0.250697</td>\n",
       "      <td>0.415931</td>\n",
       "      <td>-0.445387</td>\n",
       "      <td>-0.169576</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.084803</td>\n",
       "      <td>0.022198</td>\n",
       "      <td>-0.004809</td>\n",
       "      <td>0.226827</td>\n",
       "      <td>-0.036816</td>\n",
       "      <td>-0.034167</td>\n",
       "      <td>0.277438</td>\n",
       "      <td>-0.199405</td>\n",
       "      <td>-0.047371</td>\n",
       "      <td>-0.697044</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 102 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    author                                               text         0  \\\n",
       "0  Carroll  [Alice, begin, tired, sit, sister, bank, have,... -0.411962   \n",
       "1  Carroll  [consider, mind, hot, day, feel, sleepy, stupi... -0.323007   \n",
       "2  Carroll      [remarkable, Alice, think, way, hear, Rabbit] -0.492145   \n",
       "3  Carroll                                         [oh, dear] -0.432766   \n",
       "4  Carroll                                         [oh, dear] -0.432766   \n",
       "\n",
       "          1         2         3         4         5         6         7  ...  \\\n",
       "0 -0.530732  0.088587 -0.269412 -0.255319  0.379318 -0.398875 -0.146914  ...   \n",
       "1 -0.434219  0.066623 -0.247031 -0.199145  0.322621 -0.328407 -0.137773  ...   \n",
       "2 -0.664988  0.097490 -0.330539 -0.317640  0.492857 -0.513320 -0.171587  ...   \n",
       "3 -0.551242  0.065667 -0.326656 -0.250697  0.415931 -0.445387 -0.169576  ...   \n",
       "4 -0.551242  0.065667 -0.326656 -0.250697  0.415931 -0.445387 -0.169576  ...   \n",
       "\n",
       "         90        91        92        93        94        95        96  \\\n",
       "0 -0.070243  0.043718 -0.056762  0.201070 -0.036714 -0.039553  0.271948   \n",
       "1 -0.065762  0.041397 -0.033571  0.185754 -0.036454 -0.037165  0.237803   \n",
       "2 -0.093407  0.049211 -0.062314  0.260365 -0.041242 -0.031419  0.339620   \n",
       "3 -0.084803  0.022198 -0.004809  0.226827 -0.036816 -0.034167  0.277438   \n",
       "4 -0.084803  0.022198 -0.004809  0.226827 -0.036816 -0.034167  0.277438   \n",
       "\n",
       "         97        98        99  \n",
       "0 -0.173071 -0.046284 -0.668204  \n",
       "1 -0.157099 -0.047586 -0.555798  \n",
       "2 -0.214381 -0.052508 -0.825797  \n",
       "3 -0.199405 -0.047371 -0.697044  \n",
       "4 -0.199405 -0.047371 -0.697044  \n",
       "\n",
       "[5 rows x 102 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_arr = np.zeros((sentences.shape[0],100))\n",
    "\n",
    "for i, sentence in enumerate(sentences[\"text\"]):\n",
    "    word2vec_arr[i,:] = np.mean([model[lemma] for lemma in sentence], axis=0)\n",
    "\n",
    "word2vec_arr = pd.DataFrame(word2vec_arr)\n",
    "sentences = pd.concat([sentences[[\"author\", \"text\"]],word2vec_arr], axis=1)\n",
    "sentences.dropna(inplace=True)\n",
    "\n",
    "sentences.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FjPoGXhc_K47"
   },
   "source": [
    "This is a good dataset format. Now, you're ready to jump into the modeling step with your features. The diagram below shows where you're at the data science pipeline:\n",
    "\n",
    "![Modeling](assets/modeling.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kK3W2MKXnyeY"
   },
   "source": [
    "## Word2vec in action\n",
    "\n",
    "Notice that you now have a dataset where the columns named from *0* to *99* are the features that you'll use in the following models. Use the same models that you built in the previous checkpoints to predict the author of a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "6-repE9SnyeZ",
    "outputId": "d2e94261-0cf5-423f-f6e0-981048de397d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "Y = sentences['author']\n",
    "X = np.array(sentences.drop(['text','author'], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns a results table with the training and test scores for three models.\n",
    "def model_results(name, X, Y):\n",
    "    # Split the dataset into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.4, random_state=123)\n",
    "\n",
    "    # Models\n",
    "    lr = LogisticRegression()\n",
    "    rfc = RandomForestClassifier()\n",
    "    gbc = GradientBoostingClassifier()\n",
    "\n",
    "    lr.fit(X_train, y_train)\n",
    "    rfc.fit(X_train, y_train)\n",
    "    gbc.fit(X_train, y_train)\n",
    "    #Results Table\n",
    "    t = []\n",
    "    #type indicator for the results dictionary\n",
    "    for i in range(0, 3):\n",
    "        t.append(name)\n",
    "    #dictionary for these results.\n",
    "    d = {'type': t}\n",
    "\n",
    "\n",
    "    d['model'] = ['logistic']\n",
    "    d['test'] = [lr.score(X_test, y_test)]\n",
    "    d['train'] = [lr.score(X_train, y_train)]\n",
    "\n",
    "\n",
    "    d['model'].append('random')\n",
    "    d['test'].append(rfc.score(X_test, y_test))\n",
    "    d['train'].append(rfc.score(X_train, y_train) )\n",
    "\n",
    "    d['model'].append('gradient')\n",
    "    d['test'].append(gbc.score(X_test, y_test))\n",
    "    d['train'].append(gbc.score(X_train, y_train) )\n",
    "    \n",
    "    d = pd.DataFrame(d)\n",
    "    d.set_index(['type', 'model'], inplace=True)\n",
    "    \n",
    "    #Classification Reports\n",
    "    print(\"----------------------Logistic Regression Scores----------------------\")\n",
    "    print(classification_report(y_test, lr.predict(X_test)))\n",
    "    print(\"----------------------Random Forest Scores----------------------\")\n",
    "    print(classification_report(y_test, rfc.predict(X_test)))\n",
    "    print(\"----------------------Gradient Boosting Scores----------------------\")\n",
    "    print(classification_report(y_test, gbc.predict(X_test)))\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------Logistic Regression Scores----------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Austen       0.82      0.96      0.88      1461\n",
      "     Carroll       0.86      0.55      0.67       690\n",
      "\n",
      "    accuracy                           0.83      2151\n",
      "   macro avg       0.84      0.75      0.77      2151\n",
      "weighted avg       0.83      0.83      0.81      2151\n",
      "\n",
      "----------------------Random Forest Scores----------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Austen       0.87      0.93      0.90      1461\n",
      "     Carroll       0.83      0.69      0.75       690\n",
      "\n",
      "    accuracy                           0.85      2151\n",
      "   macro avg       0.85      0.81      0.83      2151\n",
      "weighted avg       0.85      0.85      0.85      2151\n",
      "\n",
      "----------------------Gradient Boosting Scores----------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Austen       0.87      0.92      0.90      1461\n",
      "     Carroll       0.81      0.71      0.76       690\n",
      "\n",
      "    accuracy                           0.85      2151\n",
      "   macro avg       0.84      0.82      0.83      2151\n",
      "weighted avg       0.85      0.85      0.85      2151\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>test</th>\n",
       "      <th>train</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>type</th>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">first</th>\n",
       "      <th>logistic</th>\n",
       "      <td>0.825662</td>\n",
       "      <td>0.816801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>random</th>\n",
       "      <td>0.854486</td>\n",
       "      <td>0.995040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gradient</th>\n",
       "      <td>0.854021</td>\n",
       "      <td>0.900496</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    test     train\n",
       "type  model                       \n",
       "first logistic  0.825662  0.816801\n",
       "      random    0.854486  0.995040\n",
       "      gradient  0.854021  0.900496"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#display the results. \n",
    "results = model_results('first', X, Y)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a_YUtF7snyec"
   },
   "source": [
    "The scores aren't great compared to the scores of the previous checkpoints. The main reason is the small size of your corpus.\n",
    "\n",
    "So, use word2vec vectors that are trained on a very large corpus. For this, use pretrained vectors released by Google. Google released a large set of word2vec vectors that are trained on around 100,000,000,000 words from the Google News dataset. Their corpus contains 3,000,000 words, and the word vectors that they trained have 300 features each.\n",
    "\n",
    "Download the pretrained vectors from this address: https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz. Note that the download and the following codes take some time. So, it's recommended to run the following cells in Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JpTJ_mSWnyec",
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Load Google's pretrained word2vec model.\n",
    "model_pretrained = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "    'https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vGgVddlD_K5B"
   },
   "source": [
    "Now you have the pretrained vectors in a variable called `model_pretrained`. Next, look for the vector representations of the words in your corpus. For simplicity, if a word in a sentence can't be found in the vocabulary of these pretrained vectors, you can just drop those sentences from your dataset. But you could follow alternative approaches if you like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 457
    },
    "colab_type": "code",
    "id": "K0NEP0uhohZC",
    "outputId": "ee91d765-4561-498e-933c-ebb2081cca33",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the dataset: (4619, 302)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Carroll</td>\n",
       "      <td>[Alice, begin, tired, sit, sister, bank, have,...</td>\n",
       "      <td>0.046265</td>\n",
       "      <td>0.016199</td>\n",
       "      <td>-0.036288</td>\n",
       "      <td>0.082410</td>\n",
       "      <td>-0.010284</td>\n",
       "      <td>0.015515</td>\n",
       "      <td>0.005437</td>\n",
       "      <td>-0.035947</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.066516</td>\n",
       "      <td>0.029852</td>\n",
       "      <td>-0.042609</td>\n",
       "      <td>-0.044208</td>\n",
       "      <td>-0.056998</td>\n",
       "      <td>-0.063269</td>\n",
       "      <td>0.000244</td>\n",
       "      <td>-0.085071</td>\n",
       "      <td>-0.000340</td>\n",
       "      <td>-0.064371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Carroll</td>\n",
       "      <td>[consider, mind, hot, day, feel, sleepy, stupi...</td>\n",
       "      <td>0.046331</td>\n",
       "      <td>0.020463</td>\n",
       "      <td>-0.002012</td>\n",
       "      <td>0.101565</td>\n",
       "      <td>-0.066478</td>\n",
       "      <td>-0.035698</td>\n",
       "      <td>0.045293</td>\n",
       "      <td>-0.068695</td>\n",
       "      <td>...</td>\n",
       "      <td>0.055940</td>\n",
       "      <td>0.085838</td>\n",
       "      <td>-0.067052</td>\n",
       "      <td>-0.013628</td>\n",
       "      <td>-0.027802</td>\n",
       "      <td>-0.033665</td>\n",
       "      <td>-0.023586</td>\n",
       "      <td>0.009620</td>\n",
       "      <td>0.030316</td>\n",
       "      <td>0.000908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Carroll</td>\n",
       "      <td>[remarkable, Alice, think, way, hear, Rabbit]</td>\n",
       "      <td>0.057536</td>\n",
       "      <td>-0.054036</td>\n",
       "      <td>-0.064484</td>\n",
       "      <td>0.110555</td>\n",
       "      <td>-0.021261</td>\n",
       "      <td>-0.137044</td>\n",
       "      <td>0.019613</td>\n",
       "      <td>-0.051310</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.028585</td>\n",
       "      <td>0.040721</td>\n",
       "      <td>-0.066793</td>\n",
       "      <td>0.003743</td>\n",
       "      <td>-0.084574</td>\n",
       "      <td>-0.161133</td>\n",
       "      <td>0.016876</td>\n",
       "      <td>0.036825</td>\n",
       "      <td>0.044352</td>\n",
       "      <td>-0.023448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Carroll</td>\n",
       "      <td>[oh, dear]</td>\n",
       "      <td>0.073975</td>\n",
       "      <td>0.134277</td>\n",
       "      <td>0.141357</td>\n",
       "      <td>0.256348</td>\n",
       "      <td>-0.147949</td>\n",
       "      <td>0.099670</td>\n",
       "      <td>0.077148</td>\n",
       "      <td>-0.093628</td>\n",
       "      <td>...</td>\n",
       "      <td>0.058228</td>\n",
       "      <td>0.000854</td>\n",
       "      <td>-0.094971</td>\n",
       "      <td>-0.052668</td>\n",
       "      <td>-0.091919</td>\n",
       "      <td>-0.142456</td>\n",
       "      <td>-0.053711</td>\n",
       "      <td>-0.112671</td>\n",
       "      <td>-0.148193</td>\n",
       "      <td>0.186798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Carroll</td>\n",
       "      <td>[oh, dear]</td>\n",
       "      <td>0.073975</td>\n",
       "      <td>0.134277</td>\n",
       "      <td>0.141357</td>\n",
       "      <td>0.256348</td>\n",
       "      <td>-0.147949</td>\n",
       "      <td>0.099670</td>\n",
       "      <td>0.077148</td>\n",
       "      <td>-0.093628</td>\n",
       "      <td>...</td>\n",
       "      <td>0.058228</td>\n",
       "      <td>0.000854</td>\n",
       "      <td>-0.094971</td>\n",
       "      <td>-0.052668</td>\n",
       "      <td>-0.091919</td>\n",
       "      <td>-0.142456</td>\n",
       "      <td>-0.053711</td>\n",
       "      <td>-0.112671</td>\n",
       "      <td>-0.148193</td>\n",
       "      <td>0.186798</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 302 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    author                                               text         0  \\\n",
       "0  Carroll  [Alice, begin, tired, sit, sister, bank, have,...  0.046265   \n",
       "1  Carroll  [consider, mind, hot, day, feel, sleepy, stupi...  0.046331   \n",
       "2  Carroll      [remarkable, Alice, think, way, hear, Rabbit]  0.057536   \n",
       "3  Carroll                                         [oh, dear]  0.073975   \n",
       "4  Carroll                                         [oh, dear]  0.073975   \n",
       "\n",
       "          1         2         3         4         5         6         7  ...  \\\n",
       "0  0.016199 -0.036288  0.082410 -0.010284  0.015515  0.005437 -0.035947  ...   \n",
       "1  0.020463 -0.002012  0.101565 -0.066478 -0.035698  0.045293 -0.068695  ...   \n",
       "2 -0.054036 -0.064484  0.110555 -0.021261 -0.137044  0.019613 -0.051310  ...   \n",
       "3  0.134277  0.141357  0.256348 -0.147949  0.099670  0.077148 -0.093628  ...   \n",
       "4  0.134277  0.141357  0.256348 -0.147949  0.099670  0.077148 -0.093628  ...   \n",
       "\n",
       "        290       291       292       293       294       295       296  \\\n",
       "0 -0.066516  0.029852 -0.042609 -0.044208 -0.056998 -0.063269  0.000244   \n",
       "1  0.055940  0.085838 -0.067052 -0.013628 -0.027802 -0.033665 -0.023586   \n",
       "2 -0.028585  0.040721 -0.066793  0.003743 -0.084574 -0.161133  0.016876   \n",
       "3  0.058228  0.000854 -0.094971 -0.052668 -0.091919 -0.142456 -0.053711   \n",
       "4  0.058228  0.000854 -0.094971 -0.052668 -0.091919 -0.142456 -0.053711   \n",
       "\n",
       "        297       298       299  \n",
       "0 -0.085071 -0.000340 -0.064371  \n",
       "1  0.009620  0.030316  0.000908  \n",
       "2  0.036825  0.044352 -0.023448  \n",
       "3 -0.112671 -0.148193  0.186798  \n",
       "4 -0.112671 -0.148193  0.186798  \n",
       "\n",
       "[5 rows x 302 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_arr = np.zeros((sentences.shape[0],300))\n",
    "\n",
    "for i, sentence in enumerate(sentences[\"text\"]):\n",
    "  try:\n",
    "    word2vec_arr[i,:] = np.mean([model_pretrained[lemma] for lemma in sentence], axis=0)\n",
    "  except KeyError:\n",
    "    word2vec_arr[i,:] = np.full((1,300), np.nan)\n",
    "    continue\n",
    "\n",
    "word2vec_arr = pd.DataFrame(word2vec_arr)\n",
    "sentences = pd.concat([sentences[[\"author\", \"text\"]],word2vec_arr], axis=1)\n",
    "sentences.dropna(inplace=True)\n",
    "\n",
    "print(\"Shape of the dataset: {}\".format(sentences.shape))\n",
    "sentences.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oyoFElbP_K5D"
   },
   "source": [
    "As a result, you have a dataset of 4,114 rows and 300 features (excluding the *text* and *author* columns). Now, you can run your classifiers using this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "kZkAT2q5opAJ",
    "outputId": "38258c78-a22b-417f-f12a-00266e2cd8eb",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------Logistic Regression Scores----------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Austen       0.88      0.93      0.90      1173\n",
      "     Carroll       0.86      0.77      0.81       675\n",
      "\n",
      "    accuracy                           0.87      1848\n",
      "   macro avg       0.87      0.85      0.86      1848\n",
      "weighted avg       0.87      0.87      0.87      1848\n",
      "\n",
      "----------------------Random Forest Scores----------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Austen       0.80      0.97      0.87      1173\n",
      "     Carroll       0.90      0.57      0.70       675\n",
      "\n",
      "    accuracy                           0.82      1848\n",
      "   macro avg       0.85      0.77      0.79      1848\n",
      "weighted avg       0.84      0.82      0.81      1848\n",
      "\n",
      "----------------------Gradient Boosting Scores----------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Austen       0.84      0.93      0.88      1173\n",
      "     Carroll       0.86      0.69      0.77       675\n",
      "\n",
      "    accuracy                           0.85      1848\n",
      "   macro avg       0.85      0.81      0.83      1848\n",
      "weighted avg       0.85      0.85      0.84      1848\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>test</th>\n",
       "      <th>train</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>type</th>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">first</th>\n",
       "      <th>logistic</th>\n",
       "      <td>0.825662</td>\n",
       "      <td>0.816801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>random</th>\n",
       "      <td>0.854486</td>\n",
       "      <td>0.995040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gradient</th>\n",
       "      <td>0.854021</td>\n",
       "      <td>0.900496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">second</th>\n",
       "      <th>logistic</th>\n",
       "      <td>0.871753</td>\n",
       "      <td>0.898593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>random</th>\n",
       "      <td>0.822511</td>\n",
       "      <td>0.993143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gradient</th>\n",
       "      <td>0.845779</td>\n",
       "      <td>0.961025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     test     train\n",
       "type   model                       \n",
       "first  logistic  0.825662  0.816801\n",
       "       random    0.854486  0.995040\n",
       "       gradient  0.854021  0.900496\n",
       "second logistic  0.871753  0.898593\n",
       "       random    0.822511  0.993143\n",
       "       gradient  0.845779  0.961025"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = sentences['author']\n",
    "X = np.array(sentences.drop(['text','author'], 1))\n",
    "\n",
    "#model and print the results. \n",
    "results = pd.concat([results, model_results('second', X, Y)])\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VnPsWBqd_K5F"
   },
   "source": [
    "Obviously, the scores are much better than you got with the previous word2vec vectors that you trained using your corpus. But there is still a lot of room for improvement. You can also use the pretrained vectors above as features in other models or try to gain insights from the vector compositions themselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J6BkciUjnyee",
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "# Example word2vec applications\n",
    "\n",
    "Here are some neat things that people have done with word2vec:\n",
    "\n",
    " * [Visualizing word embeddings in Jane Austen's *Pride and Prejudice*](http://blogger.ghostweather.com/2014/11/visualizing-word-embeddings-in-pride.html). Skip to the bottom to see a truly honest account of this data scientist's process.\n",
    "\n",
    " * [Tracking changes in Dutch newspapers' associations](https://www.slideshare.net/MelvinWevers/concepts-through-time-tracing-concepts-in-dutch-newspaper-discourse-using-sequential-word-vector-spaces) with words like `propaganda` and `alien`, from 1950 to 1990\n",
    "\n",
    " * [Helping customers find clothing items](http://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/) that are similar to a given item but differ on one or more characteristics\n",
    " \n",
    "Before finishing this checkpoint, there's one last vectorization method that you'll briefly cover: [GloVe](https://nlp.stanford.edu/projects/glove/). GloVe is another popular vectorization method that is similar to word2vec and developed by the researchers at Stanford University. Gensim also supports working with GloVe vectors. If you want, you can play with this method using Gensim."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Point Assignment\n",
    "\n",
    "Submit your solutions to the following tasks as a link to your Jupyter Notebook on GitHub.\n",
    "\n",
    "Train your own word2vec representations, as you did in the first example in this checkpoint. \n",
    "\n",
    "However, you need to experiment with the hyperparameters of the vectorization step. Modify the hyperparameters and run the classification models again.\n",
    "\n",
    "[Here](https://radimrehurek.com/gensim/models/word2vec.html) is additional information on word2vec parameters.\n",
    "\n",
    "Can you wrangle any improvements?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get my original sentences back in order for working with them. \n",
    "# Group into sentences\n",
    "alice_sents = [[sent, \"Carroll\"] for sent in alice_doc.sents]\n",
    "persuasion_sents = [[sent, \"Austen\"] for sent in persuasion_doc.sents]\n",
    "\n",
    "# Combine the sentences from the two novels into one DataFrame\n",
    "sentences = pd.DataFrame(alice_sents + persuasion_sents, columns = [\"text\", \"author\"])\n",
    "sentences.head()\n",
    "\n",
    "# Get rid of stop words and punctuation,\n",
    "# and lemmatize the tokens\n",
    "for i, sentence in enumerate(sentences[\"text\"]):\n",
    "    sentences.loc[i, \"text\"] = [token.lemma_ for token in sentence if not token.is_punct and not token.is_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Carroll</td>\n",
       "      <td>[Alice, begin, tired, sit, sister, bank, have,...</td>\n",
       "      <td>-0.411962</td>\n",
       "      <td>-0.530732</td>\n",
       "      <td>0.088587</td>\n",
       "      <td>-0.269412</td>\n",
       "      <td>-0.255319</td>\n",
       "      <td>0.379318</td>\n",
       "      <td>-0.398875</td>\n",
       "      <td>-0.146914</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.070243</td>\n",
       "      <td>0.043718</td>\n",
       "      <td>-0.056762</td>\n",
       "      <td>0.201070</td>\n",
       "      <td>-0.036714</td>\n",
       "      <td>-0.039553</td>\n",
       "      <td>0.271948</td>\n",
       "      <td>-0.173071</td>\n",
       "      <td>-0.046284</td>\n",
       "      <td>-0.668204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Carroll</td>\n",
       "      <td>[consider, mind, hot, day, feel, sleepy, stupi...</td>\n",
       "      <td>-0.323007</td>\n",
       "      <td>-0.434219</td>\n",
       "      <td>0.066623</td>\n",
       "      <td>-0.247031</td>\n",
       "      <td>-0.199145</td>\n",
       "      <td>0.322621</td>\n",
       "      <td>-0.328407</td>\n",
       "      <td>-0.137773</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.065762</td>\n",
       "      <td>0.041397</td>\n",
       "      <td>-0.033571</td>\n",
       "      <td>0.185754</td>\n",
       "      <td>-0.036454</td>\n",
       "      <td>-0.037165</td>\n",
       "      <td>0.237803</td>\n",
       "      <td>-0.157099</td>\n",
       "      <td>-0.047586</td>\n",
       "      <td>-0.555798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Carroll</td>\n",
       "      <td>[remarkable, Alice, think, way, hear, Rabbit]</td>\n",
       "      <td>-0.492145</td>\n",
       "      <td>-0.664988</td>\n",
       "      <td>0.097490</td>\n",
       "      <td>-0.330539</td>\n",
       "      <td>-0.317640</td>\n",
       "      <td>0.492857</td>\n",
       "      <td>-0.513320</td>\n",
       "      <td>-0.171587</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.093407</td>\n",
       "      <td>0.049211</td>\n",
       "      <td>-0.062314</td>\n",
       "      <td>0.260365</td>\n",
       "      <td>-0.041242</td>\n",
       "      <td>-0.031419</td>\n",
       "      <td>0.339620</td>\n",
       "      <td>-0.214381</td>\n",
       "      <td>-0.052508</td>\n",
       "      <td>-0.825797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Carroll</td>\n",
       "      <td>[oh, dear]</td>\n",
       "      <td>-0.432766</td>\n",
       "      <td>-0.551242</td>\n",
       "      <td>0.065667</td>\n",
       "      <td>-0.326656</td>\n",
       "      <td>-0.250697</td>\n",
       "      <td>0.415931</td>\n",
       "      <td>-0.445387</td>\n",
       "      <td>-0.169576</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.084803</td>\n",
       "      <td>0.022198</td>\n",
       "      <td>-0.004809</td>\n",
       "      <td>0.226827</td>\n",
       "      <td>-0.036816</td>\n",
       "      <td>-0.034167</td>\n",
       "      <td>0.277438</td>\n",
       "      <td>-0.199405</td>\n",
       "      <td>-0.047371</td>\n",
       "      <td>-0.697044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Carroll</td>\n",
       "      <td>[oh, dear]</td>\n",
       "      <td>-0.432766</td>\n",
       "      <td>-0.551242</td>\n",
       "      <td>0.065667</td>\n",
       "      <td>-0.326656</td>\n",
       "      <td>-0.250697</td>\n",
       "      <td>0.415931</td>\n",
       "      <td>-0.445387</td>\n",
       "      <td>-0.169576</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.084803</td>\n",
       "      <td>0.022198</td>\n",
       "      <td>-0.004809</td>\n",
       "      <td>0.226827</td>\n",
       "      <td>-0.036816</td>\n",
       "      <td>-0.034167</td>\n",
       "      <td>0.277438</td>\n",
       "      <td>-0.199405</td>\n",
       "      <td>-0.047371</td>\n",
       "      <td>-0.697044</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 102 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    author                                               text         0  \\\n",
       "0  Carroll  [Alice, begin, tired, sit, sister, bank, have,... -0.411962   \n",
       "1  Carroll  [consider, mind, hot, day, feel, sleepy, stupi... -0.323007   \n",
       "2  Carroll      [remarkable, Alice, think, way, hear, Rabbit] -0.492145   \n",
       "3  Carroll                                         [oh, dear] -0.432766   \n",
       "4  Carroll                                         [oh, dear] -0.432766   \n",
       "\n",
       "          1         2         3         4         5         6         7  ...  \\\n",
       "0 -0.530732  0.088587 -0.269412 -0.255319  0.379318 -0.398875 -0.146914  ...   \n",
       "1 -0.434219  0.066623 -0.247031 -0.199145  0.322621 -0.328407 -0.137773  ...   \n",
       "2 -0.664988  0.097490 -0.330539 -0.317640  0.492857 -0.513320 -0.171587  ...   \n",
       "3 -0.551242  0.065667 -0.326656 -0.250697  0.415931 -0.445387 -0.169576  ...   \n",
       "4 -0.551242  0.065667 -0.326656 -0.250697  0.415931 -0.445387 -0.169576  ...   \n",
       "\n",
       "         90        91        92        93        94        95        96  \\\n",
       "0 -0.070243  0.043718 -0.056762  0.201070 -0.036714 -0.039553  0.271948   \n",
       "1 -0.065762  0.041397 -0.033571  0.185754 -0.036454 -0.037165  0.237803   \n",
       "2 -0.093407  0.049211 -0.062314  0.260365 -0.041242 -0.031419  0.339620   \n",
       "3 -0.084803  0.022198 -0.004809  0.226827 -0.036816 -0.034167  0.277438   \n",
       "4 -0.084803  0.022198 -0.004809  0.226827 -0.036816 -0.034167  0.277438   \n",
       "\n",
       "         97        98        99  \n",
       "0 -0.173071 -0.046284 -0.668204  \n",
       "1 -0.157099 -0.047586 -0.555798  \n",
       "2 -0.214381 -0.052508 -0.825797  \n",
       "3 -0.199405 -0.047371 -0.697044  \n",
       "4 -0.199405 -0.047371 -0.697044  \n",
       "\n",
       "[5 rows x 102 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def config_sentences(sentences, size):\n",
    "    word2vec_arr = np.zeros((sentences.shape[0],size))\n",
    "\n",
    "    for i, sentence in enumerate(sentences[\"text\"]):\n",
    "        word2vec_arr[i,:] = np.mean([model[lemma] for lemma in sentence], axis=0)\n",
    "\n",
    "    word2vec_arr = pd.DataFrame(word2vec_arr)\n",
    "    sentences = pd.concat([sentences[[\"author\", \"text\"]],word2vec_arr], axis=1)\n",
    "    sentences.dropna(inplace=True)\n",
    "    return sentences\n",
    "\n",
    "mysentences = config_sentences(sentences, 100)\n",
    "mysentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------Logistic Regression Scores----------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Austen       0.88      0.93      0.90      1173\n",
      "     Carroll       0.86      0.77      0.81       675\n",
      "\n",
      "    accuracy                           0.87      1848\n",
      "   macro avg       0.87      0.85      0.86      1848\n",
      "weighted avg       0.87      0.87      0.87      1848\n",
      "\n",
      "----------------------Random Forest Scores----------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Austen       0.80      0.96      0.87      1173\n",
      "     Carroll       0.88      0.58      0.70       675\n",
      "\n",
      "    accuracy                           0.82      1848\n",
      "   macro avg       0.84      0.77      0.78      1848\n",
      "weighted avg       0.83      0.82      0.81      1848\n",
      "\n",
      "----------------------Gradient Boosting Scores----------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Austen       0.84      0.93      0.88      1173\n",
      "     Carroll       0.86      0.69      0.77       675\n",
      "\n",
      "    accuracy                           0.85      1848\n",
      "   macro avg       0.85      0.81      0.83      1848\n",
      "weighted avg       0.85      0.85      0.84      1848\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>test</th>\n",
       "      <th>train</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>type</th>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">first</th>\n",
       "      <th>logistic</th>\n",
       "      <td>0.825662</td>\n",
       "      <td>0.816801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>random</th>\n",
       "      <td>0.854486</td>\n",
       "      <td>0.995040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gradient</th>\n",
       "      <td>0.854021</td>\n",
       "      <td>0.900496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">second</th>\n",
       "      <th>logistic</th>\n",
       "      <td>0.871753</td>\n",
       "      <td>0.898593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>random</th>\n",
       "      <td>0.822511</td>\n",
       "      <td>0.993143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gradient</th>\n",
       "      <td>0.845779</td>\n",
       "      <td>0.961025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">cbow_mean</th>\n",
       "      <th>logistic</th>\n",
       "      <td>0.871753</td>\n",
       "      <td>0.898593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>random</th>\n",
       "      <td>0.817641</td>\n",
       "      <td>0.993143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gradient</th>\n",
       "      <td>0.845779</td>\n",
       "      <td>0.961025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        test     train\n",
       "type      model                       \n",
       "first     logistic  0.825662  0.816801\n",
       "          random    0.854486  0.995040\n",
       "          gradient  0.854021  0.900496\n",
       "second    logistic  0.871753  0.898593\n",
       "          random    0.822511  0.993143\n",
       "          gradient  0.845779  0.961025\n",
       "cbow_mean logistic  0.871753  0.898593\n",
       "          random    0.817641  0.993143\n",
       "          gradient  0.845779  0.961025"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cbow_mean = 1 \n",
    "\n",
    "# Train word2vec on the sentences\n",
    "model = gensim.models.Word2Vec(\n",
    "    mysentences[\"text\"],\n",
    "    workers=4,\n",
    "    min_count=1,\n",
    "    window=6,\n",
    "    sg=0,\n",
    "    sample=1e-3,\n",
    "    size=100,\n",
    "    hs=1,\n",
    "    cbow_mean=1\n",
    ")\n",
    "\n",
    "#model and print the results. \n",
    "results = pd.concat([results, model_results('cbow_mean', X, Y)])\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------Logistic Regression Scores----------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Austen       0.88      0.93      0.90      1173\n",
      "     Carroll       0.86      0.77      0.81       675\n",
      "\n",
      "    accuracy                           0.87      1848\n",
      "   macro avg       0.87      0.85      0.86      1848\n",
      "weighted avg       0.87      0.87      0.87      1848\n",
      "\n",
      "----------------------Random Forest Scores----------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Austen       0.80      0.97      0.88      1173\n",
      "     Carroll       0.91      0.59      0.71       675\n",
      "\n",
      "    accuracy                           0.83      1848\n",
      "   macro avg       0.86      0.78      0.80      1848\n",
      "weighted avg       0.84      0.83      0.82      1848\n",
      "\n",
      "----------------------Gradient Boosting Scores----------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Austen       0.84      0.93      0.88      1173\n",
      "     Carroll       0.86      0.69      0.77       675\n",
      "\n",
      "    accuracy                           0.85      1848\n",
      "   macro avg       0.85      0.81      0.83      1848\n",
      "weighted avg       0.85      0.85      0.84      1848\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>test</th>\n",
       "      <th>train</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>type</th>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">first</th>\n",
       "      <th>logistic</th>\n",
       "      <td>0.825662</td>\n",
       "      <td>0.816801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>random</th>\n",
       "      <td>0.854486</td>\n",
       "      <td>0.995040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gradient</th>\n",
       "      <td>0.854021</td>\n",
       "      <td>0.900496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">second</th>\n",
       "      <th>logistic</th>\n",
       "      <td>0.871753</td>\n",
       "      <td>0.898593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>random</th>\n",
       "      <td>0.822511</td>\n",
       "      <td>0.993143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gradient</th>\n",
       "      <td>0.845779</td>\n",
       "      <td>0.961025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">cbow_mean</th>\n",
       "      <th>logistic</th>\n",
       "      <td>0.871753</td>\n",
       "      <td>0.898593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>random</th>\n",
       "      <td>0.817641</td>\n",
       "      <td>0.993143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gradient</th>\n",
       "      <td>0.845779</td>\n",
       "      <td>0.961025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">w8</th>\n",
       "      <th>logistic</th>\n",
       "      <td>0.871753</td>\n",
       "      <td>0.898593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>random</th>\n",
       "      <td>0.827922</td>\n",
       "      <td>0.993143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gradient</th>\n",
       "      <td>0.845779</td>\n",
       "      <td>0.961025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        test     train\n",
       "type      model                       \n",
       "first     logistic  0.825662  0.816801\n",
       "          random    0.854486  0.995040\n",
       "          gradient  0.854021  0.900496\n",
       "second    logistic  0.871753  0.898593\n",
       "          random    0.822511  0.993143\n",
       "          gradient  0.845779  0.961025\n",
       "cbow_mean logistic  0.871753  0.898593\n",
       "          random    0.817641  0.993143\n",
       "          gradient  0.845779  0.961025\n",
       "w8        logistic  0.871753  0.898593\n",
       "          random    0.827922  0.993143\n",
       "          gradient  0.845779  0.961025"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Window Count - 8, cbow_mean on.\n",
    "# Train word2vec on the sentences\n",
    "model = gensim.models.Word2Vec(\n",
    "    mysentences[\"text\"],\n",
    "    workers=4,\n",
    "    min_count=1,\n",
    "    window=8,\n",
    "    sg=0,\n",
    "    sample=1e-3,\n",
    "    size=100,\n",
    "    hs=1,\n",
    "    cbow_mean=1\n",
    ")\n",
    "\n",
    "#model and print the results. \n",
    "results = pd.concat([results, model_results('w8', X, Y)])\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------Logistic Regression Scores----------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Austen       0.88      0.93      0.90      1173\n",
      "     Carroll       0.86      0.77      0.81       675\n",
      "\n",
      "    accuracy                           0.87      1848\n",
      "   macro avg       0.87      0.85      0.86      1848\n",
      "weighted avg       0.87      0.87      0.87      1848\n",
      "\n",
      "----------------------Random Forest Scores----------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Austen       0.81      0.97      0.88      1173\n",
      "     Carroll       0.92      0.60      0.72       675\n",
      "\n",
      "    accuracy                           0.83      1848\n",
      "   macro avg       0.86      0.78      0.80      1848\n",
      "weighted avg       0.85      0.83      0.82      1848\n",
      "\n",
      "----------------------Gradient Boosting Scores----------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Austen       0.84      0.93      0.88      1173\n",
      "     Carroll       0.86      0.69      0.77       675\n",
      "\n",
      "    accuracy                           0.85      1848\n",
      "   macro avg       0.85      0.81      0.83      1848\n",
      "weighted avg       0.85      0.85      0.84      1848\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>test</th>\n",
       "      <th>train</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>type</th>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">first</th>\n",
       "      <th>logistic</th>\n",
       "      <td>0.825662</td>\n",
       "      <td>0.816801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>random</th>\n",
       "      <td>0.854486</td>\n",
       "      <td>0.995040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gradient</th>\n",
       "      <td>0.854021</td>\n",
       "      <td>0.900496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">second</th>\n",
       "      <th>logistic</th>\n",
       "      <td>0.871753</td>\n",
       "      <td>0.898593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>random</th>\n",
       "      <td>0.822511</td>\n",
       "      <td>0.993143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gradient</th>\n",
       "      <td>0.845779</td>\n",
       "      <td>0.961025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">cbow_mean</th>\n",
       "      <th>logistic</th>\n",
       "      <td>0.871753</td>\n",
       "      <td>0.898593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>random</th>\n",
       "      <td>0.817641</td>\n",
       "      <td>0.993143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gradient</th>\n",
       "      <td>0.845779</td>\n",
       "      <td>0.961025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">w8</th>\n",
       "      <th>logistic</th>\n",
       "      <td>0.871753</td>\n",
       "      <td>0.898593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>random</th>\n",
       "      <td>0.827922</td>\n",
       "      <td>0.993143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gradient</th>\n",
       "      <td>0.845779</td>\n",
       "      <td>0.961025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">hs0</th>\n",
       "      <th>logistic</th>\n",
       "      <td>0.871753</td>\n",
       "      <td>0.898593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>random</th>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.993143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gradient</th>\n",
       "      <td>0.845779</td>\n",
       "      <td>0.961025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        test     train\n",
       "type      model                       \n",
       "first     logistic  0.825662  0.816801\n",
       "          random    0.854486  0.995040\n",
       "          gradient  0.854021  0.900496\n",
       "second    logistic  0.871753  0.898593\n",
       "          random    0.822511  0.993143\n",
       "          gradient  0.845779  0.961025\n",
       "cbow_mean logistic  0.871753  0.898593\n",
       "          random    0.817641  0.993143\n",
       "          gradient  0.845779  0.961025\n",
       "w8        logistic  0.871753  0.898593\n",
       "          random    0.827922  0.993143\n",
       "          gradient  0.845779  0.961025\n",
       "hs0       logistic  0.871753  0.898593\n",
       "          random    0.833333  0.993143\n",
       "          gradient  0.845779  0.961025"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hs = 0 everything the same as the example, but cbow_mean.\n",
    "# Train word2vec on the sentences\n",
    "model = gensim.models.Word2Vec(\n",
    "    mysentences[\"text\"],\n",
    "    workers=4,\n",
    "    min_count=1,\n",
    "    window=6,\n",
    "    sg=0,\n",
    "    sample=1e-3,\n",
    "    size=100,\n",
    "    hs=0,\n",
    "    cbow_mean=1\n",
    ")\n",
    "\n",
    "#model and print the results. \n",
    "results = pd.concat([results, model_results('hs0', X, Y)])\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------Logistic Regression Scores----------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Austen       0.88      0.93      0.90      1173\n",
      "     Carroll       0.86      0.77      0.81       675\n",
      "\n",
      "    accuracy                           0.87      1848\n",
      "   macro avg       0.87      0.85      0.86      1848\n",
      "weighted avg       0.87      0.87      0.87      1848\n",
      "\n",
      "----------------------Random Forest Scores----------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Austen       0.79      0.96      0.87      1173\n",
      "     Carroll       0.90      0.56      0.69       675\n",
      "\n",
      "    accuracy                           0.82      1848\n",
      "   macro avg       0.84      0.76      0.78      1848\n",
      "weighted avg       0.83      0.82      0.81      1848\n",
      "\n",
      "----------------------Gradient Boosting Scores----------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Austen       0.84      0.93      0.88      1173\n",
      "     Carroll       0.86      0.69      0.77       675\n",
      "\n",
      "    accuracy                           0.85      1848\n",
      "   macro avg       0.85      0.81      0.83      1848\n",
      "weighted avg       0.85      0.85      0.84      1848\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>test</th>\n",
       "      <th>train</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>type</th>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">first</th>\n",
       "      <th>logistic</th>\n",
       "      <td>0.825662</td>\n",
       "      <td>0.816801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>random</th>\n",
       "      <td>0.854486</td>\n",
       "      <td>0.995040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gradient</th>\n",
       "      <td>0.854021</td>\n",
       "      <td>0.900496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">second</th>\n",
       "      <th>logistic</th>\n",
       "      <td>0.871753</td>\n",
       "      <td>0.898593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>random</th>\n",
       "      <td>0.822511</td>\n",
       "      <td>0.993143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gradient</th>\n",
       "      <td>0.845779</td>\n",
       "      <td>0.961025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">cbow_mean</th>\n",
       "      <th>logistic</th>\n",
       "      <td>0.871753</td>\n",
       "      <td>0.898593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>random</th>\n",
       "      <td>0.817641</td>\n",
       "      <td>0.993143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gradient</th>\n",
       "      <td>0.845779</td>\n",
       "      <td>0.961025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">w8</th>\n",
       "      <th>logistic</th>\n",
       "      <td>0.871753</td>\n",
       "      <td>0.898593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>random</th>\n",
       "      <td>0.827922</td>\n",
       "      <td>0.993143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gradient</th>\n",
       "      <td>0.845779</td>\n",
       "      <td>0.961025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">hs0</th>\n",
       "      <th>logistic</th>\n",
       "      <td>0.871753</td>\n",
       "      <td>0.898593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>random</th>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.993143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gradient</th>\n",
       "      <td>0.845779</td>\n",
       "      <td>0.961025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">sg1</th>\n",
       "      <th>logistic</th>\n",
       "      <td>0.871753</td>\n",
       "      <td>0.898593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>random</th>\n",
       "      <td>0.817100</td>\n",
       "      <td>0.993143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gradient</th>\n",
       "      <td>0.845779</td>\n",
       "      <td>0.961025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        test     train\n",
       "type      model                       \n",
       "first     logistic  0.825662  0.816801\n",
       "          random    0.854486  0.995040\n",
       "          gradient  0.854021  0.900496\n",
       "second    logistic  0.871753  0.898593\n",
       "          random    0.822511  0.993143\n",
       "          gradient  0.845779  0.961025\n",
       "cbow_mean logistic  0.871753  0.898593\n",
       "          random    0.817641  0.993143\n",
       "          gradient  0.845779  0.961025\n",
       "w8        logistic  0.871753  0.898593\n",
       "          random    0.827922  0.993143\n",
       "          gradient  0.845779  0.961025\n",
       "hs0       logistic  0.871753  0.898593\n",
       "          random    0.833333  0.993143\n",
       "          gradient  0.845779  0.961025\n",
       "sg1       logistic  0.871753  0.898593\n",
       "          random    0.817100  0.993143\n",
       "          gradient  0.845779  0.961025"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sg = 1 everything the same as the example.\n",
    "# Train word2vec on the sentences\n",
    "model = gensim.models.Word2Vec(\n",
    "    mysentences[\"text\"],\n",
    "    workers=4,\n",
    "    min_count=1,\n",
    "    window=6,\n",
    "    sg=1,\n",
    "    sample=1e-3,\n",
    "    size=100,\n",
    "    hs=1\n",
    ")\n",
    "\n",
    "#model and print the results. \n",
    "results = pd.concat([results, model_results('sg1', X, Y)])\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------Logistic Regression Scores----------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Austen       0.88      0.93      0.90      1173\n",
      "     Carroll       0.86      0.77      0.81       675\n",
      "\n",
      "    accuracy                           0.87      1848\n",
      "   macro avg       0.87      0.85      0.86      1848\n",
      "weighted avg       0.87      0.87      0.87      1848\n",
      "\n",
      "----------------------Random Forest Scores----------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Austen       0.80      0.96      0.88      1173\n",
      "     Carroll       0.90      0.59      0.71       675\n",
      "\n",
      "    accuracy                           0.83      1848\n",
      "   macro avg       0.85      0.78      0.80      1848\n",
      "weighted avg       0.84      0.83      0.82      1848\n",
      "\n",
      "----------------------Gradient Boosting Scores----------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Austen       0.84      0.93      0.88      1173\n",
      "     Carroll       0.86      0.69      0.77       675\n",
      "\n",
      "    accuracy                           0.85      1848\n",
      "   macro avg       0.85      0.81      0.83      1848\n",
      "weighted avg       0.85      0.85      0.84      1848\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>test</th>\n",
       "      <th>train</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>type</th>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">first</th>\n",
       "      <th>logistic</th>\n",
       "      <td>0.825662</td>\n",
       "      <td>0.816801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>random</th>\n",
       "      <td>0.854486</td>\n",
       "      <td>0.995040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gradient</th>\n",
       "      <td>0.854021</td>\n",
       "      <td>0.900496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">second</th>\n",
       "      <th>logistic</th>\n",
       "      <td>0.871753</td>\n",
       "      <td>0.898593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>random</th>\n",
       "      <td>0.822511</td>\n",
       "      <td>0.993143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gradient</th>\n",
       "      <td>0.845779</td>\n",
       "      <td>0.961025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">cbow_mean</th>\n",
       "      <th>logistic</th>\n",
       "      <td>0.871753</td>\n",
       "      <td>0.898593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>random</th>\n",
       "      <td>0.817641</td>\n",
       "      <td>0.993143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gradient</th>\n",
       "      <td>0.845779</td>\n",
       "      <td>0.961025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">w8</th>\n",
       "      <th>logistic</th>\n",
       "      <td>0.871753</td>\n",
       "      <td>0.898593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>random</th>\n",
       "      <td>0.827922</td>\n",
       "      <td>0.993143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gradient</th>\n",
       "      <td>0.845779</td>\n",
       "      <td>0.961025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">hs0</th>\n",
       "      <th>logistic</th>\n",
       "      <td>0.871753</td>\n",
       "      <td>0.898593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>random</th>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.993143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gradient</th>\n",
       "      <td>0.845779</td>\n",
       "      <td>0.961025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">sg1</th>\n",
       "      <th>logistic</th>\n",
       "      <td>0.871753</td>\n",
       "      <td>0.898593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>random</th>\n",
       "      <td>0.817100</td>\n",
       "      <td>0.993143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gradient</th>\n",
       "      <td>0.845779</td>\n",
       "      <td>0.961025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">hs1sg1</th>\n",
       "      <th>logistic</th>\n",
       "      <td>0.871753</td>\n",
       "      <td>0.898593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>random</th>\n",
       "      <td>0.827381</td>\n",
       "      <td>0.993143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gradient</th>\n",
       "      <td>0.845779</td>\n",
       "      <td>0.961025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        test     train\n",
       "type      model                       \n",
       "first     logistic  0.825662  0.816801\n",
       "          random    0.854486  0.995040\n",
       "          gradient  0.854021  0.900496\n",
       "second    logistic  0.871753  0.898593\n",
       "          random    0.822511  0.993143\n",
       "          gradient  0.845779  0.961025\n",
       "cbow_mean logistic  0.871753  0.898593\n",
       "          random    0.817641  0.993143\n",
       "          gradient  0.845779  0.961025\n",
       "w8        logistic  0.871753  0.898593\n",
       "          random    0.827922  0.993143\n",
       "          gradient  0.845779  0.961025\n",
       "hs0       logistic  0.871753  0.898593\n",
       "          random    0.833333  0.993143\n",
       "          gradient  0.845779  0.961025\n",
       "sg1       logistic  0.871753  0.898593\n",
       "          random    0.817100  0.993143\n",
       "          gradient  0.845779  0.961025\n",
       "hs1sg1    logistic  0.871753  0.898593\n",
       "          random    0.827381  0.993143\n",
       "          gradient  0.845779  0.961025"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train word2vec on the sentences\n",
    "model = gensim.models.Word2Vec(\n",
    "    mysentences[\"text\"],\n",
    "    workers=4,\n",
    "    min_count=1,\n",
    "    window=6,\n",
    "    sg=1,\n",
    "    sample=1e-3,\n",
    "    size=100,\n",
    "    hs=1\n",
    ")\n",
    "\n",
    "#model and print the results. \n",
    "results = pd.concat([results, model_results('hs1sg1', X, Y)])\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------Logistic Regression Scores----------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Austen       0.88      0.93      0.90      1173\n",
      "     Carroll       0.86      0.77      0.81       675\n",
      "\n",
      "    accuracy                           0.87      1848\n",
      "   macro avg       0.87      0.85      0.86      1848\n",
      "weighted avg       0.87      0.87      0.87      1848\n",
      "\n",
      "----------------------Random Forest Scores----------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Austen       0.79      0.97      0.87      1173\n",
      "     Carroll       0.91      0.57      0.70       675\n",
      "\n",
      "    accuracy                           0.82      1848\n",
      "   macro avg       0.85      0.77      0.78      1848\n",
      "weighted avg       0.84      0.82      0.81      1848\n",
      "\n",
      "----------------------Gradient Boosting Scores----------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Austen       0.84      0.93      0.88      1173\n",
      "     Carroll       0.86      0.69      0.77       675\n",
      "\n",
      "    accuracy                           0.85      1848\n",
      "   macro avg       0.85      0.81      0.83      1848\n",
      "weighted avg       0.85      0.85      0.84      1848\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>test</th>\n",
       "      <th>train</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>type</th>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">first</th>\n",
       "      <th>logistic</th>\n",
       "      <td>0.825662</td>\n",
       "      <td>0.816801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>random</th>\n",
       "      <td>0.854486</td>\n",
       "      <td>0.995040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gradient</th>\n",
       "      <td>0.854021</td>\n",
       "      <td>0.900496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">second</th>\n",
       "      <th>logistic</th>\n",
       "      <td>0.871753</td>\n",
       "      <td>0.898593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>random</th>\n",
       "      <td>0.822511</td>\n",
       "      <td>0.993143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gradient</th>\n",
       "      <td>0.845779</td>\n",
       "      <td>0.961025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">cbow_mean</th>\n",
       "      <th>logistic</th>\n",
       "      <td>0.871753</td>\n",
       "      <td>0.898593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>random</th>\n",
       "      <td>0.817641</td>\n",
       "      <td>0.993143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gradient</th>\n",
       "      <td>0.845779</td>\n",
       "      <td>0.961025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">w8</th>\n",
       "      <th>logistic</th>\n",
       "      <td>0.871753</td>\n",
       "      <td>0.898593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>random</th>\n",
       "      <td>0.827922</td>\n",
       "      <td>0.993143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gradient</th>\n",
       "      <td>0.845779</td>\n",
       "      <td>0.961025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">hs0</th>\n",
       "      <th>logistic</th>\n",
       "      <td>0.871753</td>\n",
       "      <td>0.898593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>random</th>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.993143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gradient</th>\n",
       "      <td>0.845779</td>\n",
       "      <td>0.961025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">sg1</th>\n",
       "      <th>logistic</th>\n",
       "      <td>0.871753</td>\n",
       "      <td>0.898593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>random</th>\n",
       "      <td>0.817100</td>\n",
       "      <td>0.993143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gradient</th>\n",
       "      <td>0.845779</td>\n",
       "      <td>0.961025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">hs1sg1</th>\n",
       "      <th>logistic</th>\n",
       "      <td>0.871753</td>\n",
       "      <td>0.898593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>random</th>\n",
       "      <td>0.827381</td>\n",
       "      <td>0.993143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gradient</th>\n",
       "      <td>0.845779</td>\n",
       "      <td>0.961025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">cb1worker6</th>\n",
       "      <th>logistic</th>\n",
       "      <td>0.871753</td>\n",
       "      <td>0.898593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>random</th>\n",
       "      <td>0.820346</td>\n",
       "      <td>0.993143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gradient</th>\n",
       "      <td>0.845779</td>\n",
       "      <td>0.961025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         test     train\n",
       "type       model                       \n",
       "first      logistic  0.825662  0.816801\n",
       "           random    0.854486  0.995040\n",
       "           gradient  0.854021  0.900496\n",
       "second     logistic  0.871753  0.898593\n",
       "           random    0.822511  0.993143\n",
       "           gradient  0.845779  0.961025\n",
       "cbow_mean  logistic  0.871753  0.898593\n",
       "           random    0.817641  0.993143\n",
       "           gradient  0.845779  0.961025\n",
       "w8         logistic  0.871753  0.898593\n",
       "           random    0.827922  0.993143\n",
       "           gradient  0.845779  0.961025\n",
       "hs0        logistic  0.871753  0.898593\n",
       "           random    0.833333  0.993143\n",
       "           gradient  0.845779  0.961025\n",
       "sg1        logistic  0.871753  0.898593\n",
       "           random    0.817100  0.993143\n",
       "           gradient  0.845779  0.961025\n",
       "hs1sg1     logistic  0.871753  0.898593\n",
       "           random    0.827381  0.993143\n",
       "           gradient  0.845779  0.961025\n",
       "cb1worker6 logistic  0.871753  0.898593\n",
       "           random    0.820346  0.993143\n",
       "           gradient  0.845779  0.961025"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cbow_mean = 1, works = 6\n",
    "\n",
    "# Train word2vec on the sentences\n",
    "model = gensim.models.Word2Vec(\n",
    "    mysentences[\"text\"],\n",
    "    workers=6,\n",
    "    min_count=1,\n",
    "    window=6,\n",
    "    sg=0,\n",
    "    sample=1e-3,\n",
    "    size=100,\n",
    "    hs=1,\n",
    "    cbow_mean=1\n",
    ")\n",
    "\n",
    "#model and print the results. \n",
    "results = pd.concat([results, model_results('cb1worker6', X, Y)])\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------Logistic Regression Scores----------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Austen       0.88      0.93      0.90      1173\n",
      "     Carroll       0.86      0.77      0.81       675\n",
      "\n",
      "    accuracy                           0.87      1848\n",
      "   macro avg       0.87      0.85      0.86      1848\n",
      "weighted avg       0.87      0.87      0.87      1848\n",
      "\n",
      "----------------------Random Forest Scores----------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Austen       0.81      0.96      0.88      1173\n",
      "     Carroll       0.90      0.60      0.72       675\n",
      "\n",
      "    accuracy                           0.83      1848\n",
      "   macro avg       0.86      0.78      0.80      1848\n",
      "weighted avg       0.84      0.83      0.82      1848\n",
      "\n",
      "----------------------Gradient Boosting Scores----------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Austen       0.84      0.93      0.88      1173\n",
      "     Carroll       0.86      0.69      0.77       675\n",
      "\n",
      "    accuracy                           0.85      1848\n",
      "   macro avg       0.85      0.81      0.83      1848\n",
      "weighted avg       0.85      0.85      0.84      1848\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>test</th>\n",
       "      <th>train</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>type</th>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">first</th>\n",
       "      <th>logistic</th>\n",
       "      <td>0.825662</td>\n",
       "      <td>0.816801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>random</th>\n",
       "      <td>0.854486</td>\n",
       "      <td>0.995040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gradient</th>\n",
       "      <td>0.854021</td>\n",
       "      <td>0.900496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">second</th>\n",
       "      <th>logistic</th>\n",
       "      <td>0.871753</td>\n",
       "      <td>0.898593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>random</th>\n",
       "      <td>0.822511</td>\n",
       "      <td>0.993143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gradient</th>\n",
       "      <td>0.845779</td>\n",
       "      <td>0.961025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">cbow_mean</th>\n",
       "      <th>logistic</th>\n",
       "      <td>0.871753</td>\n",
       "      <td>0.898593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>random</th>\n",
       "      <td>0.817641</td>\n",
       "      <td>0.993143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gradient</th>\n",
       "      <td>0.845779</td>\n",
       "      <td>0.961025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">w8</th>\n",
       "      <th>logistic</th>\n",
       "      <td>0.871753</td>\n",
       "      <td>0.898593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>random</th>\n",
       "      <td>0.827922</td>\n",
       "      <td>0.993143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gradient</th>\n",
       "      <td>0.845779</td>\n",
       "      <td>0.961025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">hs0</th>\n",
       "      <th>logistic</th>\n",
       "      <td>0.871753</td>\n",
       "      <td>0.898593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>random</th>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.993143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gradient</th>\n",
       "      <td>0.845779</td>\n",
       "      <td>0.961025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">sg1</th>\n",
       "      <th>logistic</th>\n",
       "      <td>0.871753</td>\n",
       "      <td>0.898593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>random</th>\n",
       "      <td>0.817100</td>\n",
       "      <td>0.993143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gradient</th>\n",
       "      <td>0.845779</td>\n",
       "      <td>0.961025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">hs1sg1</th>\n",
       "      <th>logistic</th>\n",
       "      <td>0.871753</td>\n",
       "      <td>0.898593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>random</th>\n",
       "      <td>0.827381</td>\n",
       "      <td>0.993143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gradient</th>\n",
       "      <td>0.845779</td>\n",
       "      <td>0.961025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">cb1worker6</th>\n",
       "      <th>logistic</th>\n",
       "      <td>0.871753</td>\n",
       "      <td>0.898593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>random</th>\n",
       "      <td>0.820346</td>\n",
       "      <td>0.993143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gradient</th>\n",
       "      <td>0.845779</td>\n",
       "      <td>0.961025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">cbow_mean_min_count3</th>\n",
       "      <th>logistic</th>\n",
       "      <td>0.871753</td>\n",
       "      <td>0.898593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>random</th>\n",
       "      <td>0.831710</td>\n",
       "      <td>0.993143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gradient</th>\n",
       "      <td>0.845779</td>\n",
       "      <td>0.961025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   test     train\n",
       "type                 model                       \n",
       "first                logistic  0.825662  0.816801\n",
       "                     random    0.854486  0.995040\n",
       "                     gradient  0.854021  0.900496\n",
       "second               logistic  0.871753  0.898593\n",
       "                     random    0.822511  0.993143\n",
       "                     gradient  0.845779  0.961025\n",
       "cbow_mean            logistic  0.871753  0.898593\n",
       "                     random    0.817641  0.993143\n",
       "                     gradient  0.845779  0.961025\n",
       "w8                   logistic  0.871753  0.898593\n",
       "                     random    0.827922  0.993143\n",
       "                     gradient  0.845779  0.961025\n",
       "hs0                  logistic  0.871753  0.898593\n",
       "                     random    0.833333  0.993143\n",
       "                     gradient  0.845779  0.961025\n",
       "sg1                  logistic  0.871753  0.898593\n",
       "                     random    0.817100  0.993143\n",
       "                     gradient  0.845779  0.961025\n",
       "hs1sg1               logistic  0.871753  0.898593\n",
       "                     random    0.827381  0.993143\n",
       "                     gradient  0.845779  0.961025\n",
       "cb1worker6           logistic  0.871753  0.898593\n",
       "                     random    0.820346  0.993143\n",
       "                     gradient  0.845779  0.961025\n",
       "cbow_mean_min_count3 logistic  0.871753  0.898593\n",
       "                     random    0.831710  0.993143\n",
       "                     gradient  0.845779  0.961025"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cbow_mean = 1 \n",
    "\n",
    "# Train word2vec on the sentences\n",
    "model = gensim.models.Word2Vec(\n",
    "    mysentences[\"text\"],\n",
    "    workers=4,\n",
    "    min_count=3,\n",
    "    window=6,\n",
    "    sg=0,\n",
    "    sample=1e-3,\n",
    "    size=100,\n",
    "    hs=1,\n",
    "    cbow_mean=1\n",
    ")\n",
    "\n",
    "#model and print the results. \n",
    "results = pd.concat([results, model_results('cbow_mean_min_count3', X, Y)])\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='type,model'>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "results.nlargest(5, 'test').plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using either skip_grams or c_bow mean, I can acheive a higher accuracy on the logistic test scores. This also provides the best precision and recall scores across the board on the dataset. Additionally, I find that these scores mimic the logistic results seen in the google pre-trained model; which also had its best results return as logistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "6.feature_engineering_3.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "96px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
